{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.display import HTML\n",
    "from pyro.infer import SVI, Predictive, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoMultivariateNormal, init_to_mean\n",
    "\n",
    "from collab2.foraging.fish import StochasticFish_IndependentRates,animate_trajectories\n",
    "from collab2.foraging.toolkit import (\n",
    "    animate_predictors,\n",
    "    dataObject,\n",
    "    derive_predictors_and_scores,\n",
    "    plot_coefs,\n",
    "    rescale_to_grid,\n",
    "    subsample_frames_constant_frame_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common parameters for simulations and predictor derivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schooling simulations params\n",
    "N = 6\n",
    "arena_size = 100\n",
    "Tmax = 10\n",
    "L = 80\n",
    "sigma_v = 5\n",
    "sigma_t = 0.2\n",
    "vscale = 10\n",
    "rate = 10\n",
    "\n",
    "# Predictor/score derivation params \n",
    "grid_size = 100\n",
    "leeway = 10\n",
    "window_size_multiplier = 0.2\n",
    "grid_sampling_fraction = 1\n",
    "\n",
    "score_kwargs = {\n",
    "    \"nextStep_sublinear\" : {\"nonlinearity_exponent\" : 0.5},\n",
    "    \"nextStepExponential\" : {\"decay_length\" : 5},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to generate derivedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictors_from_sim(\n",
    "    sim,\n",
    "    grid_size=100,\n",
    "    frames_skipped=1,\n",
    "    leeway=10,\n",
    "    verbose=False,\n",
    "    grid_sampling_fraction=1,\n",
    "    normalize=False,\n",
    "    window_size_multiplier = 0.5,\n",
    "    score_kwargs = None\n",
    "):\n",
    "\n",
    "    # choose frames_skipped based on dt and vscale if not provided\n",
    "    if frames_skipped is None:\n",
    "        frames_skipped = max(1, int(0.01 * sim.arena_size / (sim.vscale * sim.dt)))\n",
    "\n",
    "    print(f\"Time points skipped for subsampling : {frames_skipped}\")\n",
    "\n",
    "    # get DataFrame\n",
    "    x = sim.trajectories[:, :, 0].reshape(-1)\n",
    "    y = sim.trajectories[:, :, 1].reshape(-1)\n",
    "    num_frames = sim.trajectories.shape[1]\n",
    "    num_foragers = sim.trajectories.shape[0]\n",
    "    time = np.tile(np.arange(num_frames), num_foragers)\n",
    "    foragers = np.concatenate([i * np.ones(num_frames) for i in range(num_foragers)])\n",
    "\n",
    "    data = {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"time\": time,\n",
    "        \"forager\": foragers,\n",
    "    }\n",
    "\n",
    "    foragersDF = pd.DataFrame(data)\n",
    "\n",
    "    # discretize to grid\n",
    "    gridMin = -sim.arena_size\n",
    "    gridMax = sim.arena_size\n",
    "    foragersDF_scaled = rescale_to_grid(\n",
    "        foragersDF, size=grid_size, gridMin=gridMin, gridMax=gridMax\n",
    "    )\n",
    "\n",
    "    foragersDF_scaled_subsampled = subsample_frames_constant_frame_rate(foragersDF_scaled,frame_spacing=frames_skipped)\n",
    "    \n",
    "    # create foragers object\n",
    "    foragers_object = dataObject(\n",
    "        foragersDF_scaled_subsampled,\n",
    "        grid_size=grid_size,\n",
    "    )\n",
    "\n",
    "    # define circular grid constraint function\n",
    "    def circular_constraint_func(grid, c_x, c_y, R):\n",
    "        ind = ((grid[\"x\"] - c_x) ** 2 + (grid[\"y\"] - c_y) ** 2) < R**2\n",
    "        return grid.loc[ind]\n",
    "\n",
    "    grid_constraint_params = {\n",
    "        \"c_x\": grid_size / 2,\n",
    "        \"c_y\": grid_size / 2,\n",
    "        \"R\": grid_size / 2,\n",
    "    }\n",
    "\n",
    "    # choose kwargs for predictors\n",
    "    # for now, exactly \"matched\" to sim parameters.\n",
    "    # CAUTION : subsampling and discretizing to grid changes time and length units!!\n",
    "    # need to appropriately rescale these quantities where applicable.\n",
    "    # eg, velocities used to calculate predictors are in units of (grid unit)/(frame interval)\n",
    "    # therefore, need to rescale sigmas by multiplying with (len_conversion_param)/(time_conversion_param)\n",
    "\n",
    "    # However, rescaling leads to extremely small sigmas.\n",
    "    # make sigmas larger to allow for more leeway?\n",
    "\n",
    "    # plan:\n",
    "    #  if only one of v or pC interactions : use same params for predictors for both\n",
    "    #  if both present : use respective params for predictors\n",
    "    #  if neither present: use L=grid_size/3 and sigmas same as diffusion\n",
    "    #  if diffusion not present: use avg of v and pC params\n",
    "\n",
    "    sim_parameters = sim.interaction_params\n",
    "    length_conversion_param = grid_size / (\n",
    "        2 * sim.arena_size\n",
    "    )  # 1 grid unit = length_conversion_param * sim length unit\n",
    "    time_conversion_param = 1 / (\n",
    "        frames_skipped * sim.dt\n",
    "    )  # 1 frame interval = time_conversion_param * sim time unit\n",
    "\n",
    "    foragers_object.length_conversion_param = length_conversion_param\n",
    "    foragers_object.time_conversion_param = time_conversion_param\n",
    "\n",
    "    print(f\"1 grid unit = {length_conversion_param} * simulation length unit\")\n",
    "    print(f\"1 frame interval = {time_conversion_param} *  simulation time unit\")\n",
    "\n",
    "    try:\n",
    "        L_vicsek = length_conversion_param * np.mean(\n",
    "            sim_parameters[\"vicsek\"][\"interaction_length\"]\n",
    "        )\n",
    "        sigma_v_vicsek = (\n",
    "            leeway\n",
    "            * length_conversion_param\n",
    "            / time_conversion_param\n",
    "            * np.mean(sim_parameters[\"vicsek\"][\"sigma_v\"])\n",
    "        )\n",
    "        sigma_t_vicsek = np.mean(sim_parameters[\"vicsek\"][\"sigma_t\"])\n",
    "\n",
    "    except KeyError:\n",
    "        L_vicsek = np.nan\n",
    "        sigma_v_vicsek = np.nan\n",
    "        sigma_t_vicsek = np.nan\n",
    "\n",
    "    try:\n",
    "        L_pC = length_conversion_param * np.mean(\n",
    "            sim.interaction_params[\"pairwiseCopying\"][\"interaction_length\"]\n",
    "        )\n",
    "        sigma_v_pC = (\n",
    "            leeway\n",
    "            * length_conversion_param\n",
    "            / time_conversion_param\n",
    "            * np.mean(sim.interaction_params[\"pairwiseCopying\"][\"sigma_v\"])\n",
    "        )\n",
    "        sigma_t_pC = np.mean(sim.interaction_params[\"pairwiseCopying\"][\"sigma_t\"])\n",
    "\n",
    "    except KeyError:\n",
    "        L_pC = np.nan\n",
    "        sigma_v_pC = np.nan\n",
    "        sigma_t_pC = np.nan\n",
    "\n",
    "    try:\n",
    "        sigma_v_diff = (\n",
    "            leeway\n",
    "            * length_conversion_param\n",
    "            / time_conversion_param\n",
    "            * np.mean(sim.interaction_params[\"diffusion\"][\"sigma_v\"])\n",
    "        )\n",
    "        sigma_t_diff = np.mean(sim.interaction_params[\"diffusion\"][\"sigma_t\"])\n",
    "\n",
    "    except KeyError:\n",
    "        sigma_v_diff = np.nan\n",
    "        sigma_t_diff = np.nan\n",
    "\n",
    "    if np.logical_xor(np.isnan(L_vicsek), np.isnan(L_pC)):\n",
    "        if np.isnan(L_vicsek):\n",
    "            L_vicsek = L_pC\n",
    "            sigma_v_vicsek = sigma_v_pC\n",
    "            sigma_t_vicsek = sigma_t_pC\n",
    "\n",
    "        else:\n",
    "            L_pC = L_vicsek\n",
    "            sigma_v_pC = sigma_v_vicsek\n",
    "            sigma_t_pC = sigma_t_vicsek\n",
    "\n",
    "    elif np.isnan(L_vicsek) and np.isnan(L_pC):\n",
    "        L_vicsek = grid_size / 3\n",
    "        L_pC = grid_size / 3\n",
    "        sigma_v_vicsek = sigma_v_diff\n",
    "        sigma_v_pC = sigma_v_diff\n",
    "        sigma_t_vicsek = sigma_t_diff\n",
    "        sigma_t_pC = sigma_t_diff\n",
    "\n",
    "    if np.isnan(sigma_v_diff):\n",
    "        sigma_v_diff = np.mean([sigma_v_vicsek, sigma_v_pC])\n",
    "        sigma_t_diff = np.mean([sigma_t_vicsek, sigma_t_pC])\n",
    "\n",
    "    local_windows_kwargs = {\n",
    "        \"window_size\": np.maximum(L_pC, L_vicsek) * window_size_multiplier,\n",
    "        \"sampling_fraction\": grid_sampling_fraction,\n",
    "        \"skip_incomplete_frames\": True,\n",
    "        \"grid_constraint\": circular_constraint_func,\n",
    "        \"grid_constraint_params\": grid_constraint_params,\n",
    "    }\n",
    "\n",
    "    predictor_kwargs = {\n",
    "        \"vicsek\": {\n",
    "            \"dt\": 1,\n",
    "            \"sigma_v\": sigma_v_vicsek,\n",
    "            \"sigma_t\": sigma_t_vicsek,\n",
    "            \"interaction_length\": L_vicsek,\n",
    "        },\n",
    "        \"pairwiseCopying\": {\n",
    "            \"dt\": 1,\n",
    "            \"sigma_v\": sigma_v_pC,\n",
    "            \"sigma_t\": sigma_t_pC,\n",
    "            \"interaction_length\": L_pC,\n",
    "        },\n",
    "        \"velocityDiffusion\": {\n",
    "            \"dt\": 1,\n",
    "            \"sigma_v\": sigma_v_diff,\n",
    "            \"sigma_t\": sigma_t_diff,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if score_kwargs is None:\n",
    "        score_kwargs = {\n",
    "            \"nextStep_sublinear\" : {\"nonlinearity_exponent\" : 0.5},\n",
    "            \"nextStepExponential\" : {\"decay_length\" : 1},\n",
    "        }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Predictor kwargs:\")\n",
    "        pprint(predictor_kwargs)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # derive predictors and scores\n",
    "    derivedDF = derive_predictors_and_scores(\n",
    "        foragers_object,\n",
    "        local_windows_kwargs=local_windows_kwargs,\n",
    "        predictor_kwargs=predictor_kwargs,\n",
    "        score_kwargs=score_kwargs,\n",
    "        dropna=True,\n",
    "        add_scaled_values=normalize,\n",
    "    )\n",
    "    return foragers_object, derivedDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors_for_inference(\n",
    "    derivedDF: pd.DataFrame, predictor_names: List[str], score_names: List[str]\n",
    "):\n",
    "    predictors = {}\n",
    "    for name in predictor_names:\n",
    "        predictors[name] = torch.tensor(derivedDF[name].values)\n",
    "\n",
    "    scores = {}\n",
    "    for name in score_names:\n",
    "        scores[name] = torch.tensor(derivedDF[name].values)\n",
    "\n",
    "    return predictors, scores\n",
    "\n",
    "\n",
    "def run_svi_and_get_samples(\n",
    "    model, predictors, score, num_svi_iters=600, verbose=False, num_samples=1000\n",
    "):\n",
    "    guide = AutoMultivariateNormal(model, init_loc_fn=init_to_mean)\n",
    "    svi = SVI(model, guide, optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "\n",
    "    iterations = []\n",
    "    losses = []\n",
    "\n",
    "    logging.info(f\"Starting SVI inference with {num_svi_iters} iterations.\")\n",
    "    start_time = time.time()\n",
    "    pyro.clear_param_store()\n",
    "    for i in range(num_svi_iters):\n",
    "        elbo = svi.step(predictors=predictors, score=score)\n",
    "        iterations.append(i)\n",
    "        losses.append(elbo)\n",
    "        if i % 200 == 0:\n",
    "            logging.info(\"Step {}, elbo loss: {}\".format(i, elbo))\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info(\"SVI inference completed in %.2f seconds.\", elapsed_time)\n",
    "\n",
    "    if verbose:\n",
    "        fig = px.line(\n",
    "            x=iterations, y=losses, title=\"ELBO loss\", template=\"presentation\"\n",
    "        )\n",
    "        labels = {\"iterations\": \"iteration\", \"losses\": \"loss\"}\n",
    "        fig.update_xaxes(showgrid=False, title_text=labels[\"iterations\"])\n",
    "        fig.update_yaxes(showgrid=False, title_text=labels[\"losses\"])\n",
    "        fig.update_layout(width=700)\n",
    "        fig.show()\n",
    "\n",
    "    site_names = [f\"weight_{key}\" for key in predictors.keys()]\n",
    "    predictive = Predictive(\n",
    "        model=model,\n",
    "        guide=guide,\n",
    "        num_samples=num_samples,\n",
    "        parallel=False,\n",
    "        return_sites=site_names,\n",
    "    )\n",
    "    samples = predictive(predictors=predictors, score=score)\n",
    "    return guide, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyro model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FishModel(pyro.nn.PyroModule):\n",
    "    def __init__(self, predictors: dict, score: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.predictors = predictors\n",
    "        self.score = score\n",
    "        self.n = len(score)\n",
    "\n",
    "    def forward(self, predictors=None, score=None):\n",
    "        if predictors is None:\n",
    "            predictors = self.predictors\n",
    "\n",
    "        size = len(next(iter(predictors.values())))\n",
    "\n",
    "        weights = {}\n",
    "        sigmas = {}\n",
    "        for key in predictors.keys():\n",
    "            weights[key] = pyro.sample(f\"weight_{key}\", dist.Normal(0.0, 1))\n",
    "            sigmas[key] = pyro.sample(f\"sigma_{key}\", dist.Exponential(10.0))\n",
    "\n",
    "        weights[\"b\"] = pyro.sample(\"b\", dist.Normal(0.5, 0.3))\n",
    "        sigmas[\"bs\"] = pyro.sample(\"bs\", dist.Exponential(10.0))\n",
    "\n",
    "        with pyro.plate(\"data\", size):\n",
    "            sigma = pyro.deterministic(\n",
    "                \"sigma\",\n",
    "                sum([predictors[key] * sigmas[key] for key in predictors.keys()])\n",
    "                + sigmas[\"bs\"],\n",
    "            )\n",
    "\n",
    "            assert torch.all(sigma > 0), \"Sigma is not positive\"\n",
    "\n",
    "            mean = pyro.deterministic(\n",
    "                \"mean\",\n",
    "                sum([predictors[key] * weights[key] for key in predictors.keys()])\n",
    "                + weights[\"b\"],\n",
    "            )\n",
    "\n",
    "            observed_score = None\n",
    "            if score is not None:\n",
    "                observed_score = next(iter(score.values()))\n",
    "\n",
    "            pyro.sample(\"predicted_score\", dist.Normal(mean, sigma), obs=observed_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting score predictions is slower. only do that for a fraction of datapoints\n",
    "# and fewer samples!\n",
    "def evaluate_performance(\n",
    "    model, guide, predictors, score, num_samples=100, sampling_fraction=1\n",
    "):\n",
    "    score_name = next(iter(score.keys()))\n",
    "\n",
    "    sub_size = int(score[score_name].size(0) * sampling_fraction)\n",
    "    randinds = torch.randint(0, score[score_name].size(0), (sub_size,))\n",
    "\n",
    "    # downsample predictors and score\n",
    "    score_sub = {score_name: score[score_name][randinds]}\n",
    "\n",
    "    predictors_sub = {}\n",
    "    for key, value in predictors.items():\n",
    "        predictors_sub[key] = value[randinds]\n",
    "\n",
    "    predictive = Predictive(\n",
    "        model=model,\n",
    "        guide=guide,\n",
    "        num_samples=num_samples,\n",
    "        parallel=False,\n",
    "        return_sites=[\"predicted_score\"],\n",
    "    )\n",
    "    samples = predictive(predictors=predictors_sub, score=None)\n",
    "\n",
    "    predictions = samples[\"predicted_score\"]\n",
    "\n",
    "    predictions_mean = predictions.mean(dim=0)\n",
    "    predictions_lower = predictions.quantile(0.025, dim=0)\n",
    "    predictions_upper = predictions.quantile(0.975, dim=0)\n",
    "\n",
    "    coverage = (\n",
    "        (\n",
    "            (predictions_lower <= score_sub[score_name])\n",
    "            & (score_sub[score_name] <= predictions_upper)\n",
    "        )\n",
    "        .float()\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    observed_mean = score_sub[score_name].mean()\n",
    "\n",
    "    residuals = score_sub[score_name] - predictions_mean\n",
    "\n",
    "    mae = (torch.abs(residuals)).mean()\n",
    "\n",
    "    rsquared = 1 - (\n",
    "        torch.sum(residuals**2)\n",
    "        / torch.sum((score_sub[score_name] - observed_mean) ** 2)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    heatmap = ax[0].hexbin(\n",
    "        score_sub[score_name],\n",
    "        predictions_mean,\n",
    "        gridsize=100,\n",
    "        cmap=\"Blues\",\n",
    "        norm=mpl.colors.LogNorm(),\n",
    "    )\n",
    "    fig.colorbar(heatmap, ax=ax[0])\n",
    "    ax[0].set_title(\"Ratio of outcomes within 95% CI: {:.2f}\".format(coverage.item()))\n",
    "    ax[0].set_xlabel(\"observed values\")\n",
    "    ax[0].set_ylabel(\"mean predicted values\")\n",
    "\n",
    "    ax[1].hist(residuals.detach().numpy(), bins=50)\n",
    "    ax[1].set_title(f\"Residuals, MAE: {mae.item():.2f}, R²: {rsquared.item():.2f}\")\n",
    "    ax[1].set_xlabel(\"residuals\")\n",
    "    ax[1].set_ylabel(\"count\")\n",
    "\n",
    "    # print(f\"Residuals, MAE: {mae.item():.2f}, R²: {rsquared.item():.2f}\")\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "    sns.despine()\n",
    "    fig.suptitle(\"Model evaluation\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(predictors, scores, title):\n",
    "    n_plots = len(predictors) * len(scores)\n",
    "    #print(n_plots)\n",
    "    ncols = 3\n",
    "    nrows = np.ceil(n_plots / ncols).astype(int)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 3 * nrows))\n",
    "\n",
    "    plot_index = 0\n",
    "    for i, predictor_name in enumerate(predictors.keys()):\n",
    "        for j, score_name in enumerate(scores.keys()):\n",
    "\n",
    "            if nrows > 1:\n",
    "                r = plot_index // ncols\n",
    "                c = plot_index % ncols\n",
    "                #print([r,c])\n",
    "                ax = axes[r, c]\n",
    "            else:\n",
    "                ax = axes[plot_index]\n",
    "\n",
    "            heatmap = ax.hexbin(\n",
    "                predictors[predictor_name],\n",
    "                scores[score_name],\n",
    "                gridsize=100,\n",
    "                cmap=\"Blues\",\n",
    "                norm=mpl.colors.LogNorm(),\n",
    "            )\n",
    "            fig.colorbar(heatmap, ax=ax)\n",
    "            ax.set_xlabel(f\"{predictor_name} predictor\")\n",
    "            ax.set_ylabel(f\"{score_name} score\")\n",
    "            # ax.set_aspect(\"equal\")\n",
    "            plot_index += 1\n",
    "\n",
    "    # remove unused plots\n",
    "    if n_plots % ncols:\n",
    "        for ind in range(n_plots % ncols, ncols):\n",
    "            if nrows > 1:\n",
    "                fig.delaxes(axes[nrows-1,ind])\n",
    "            else:\n",
    "                fig.delaxes(axes[ind])\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "def plot_distributions(derivedDF, column_names):\n",
    "    fig, ax = plt.subplots(1, len(column_names), figsize=(3 * len(column_names), 3))\n",
    "\n",
    "    for i, column in enumerate(column_names):\n",
    "        ax[i].hist(derivedDF[column], bins=20, density=True, alpha=1)\n",
    "        ax[i].set_yscale(\"log\")\n",
    "        ax[i].set_ylim([10**-3, 10**2])\n",
    "        ax[i].set_xlabel(column)\n",
    "        ax[i].set_ylabel(\"PDF\")\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vicsek interactions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging time step: 0.0016666666666666668\n",
      "Velocity scale for initialization:  10.00\n",
      "Total time steps: 6001\n",
      "Average number of vicsek interactions before leaving interaction radius: 80\n",
      "Time-steps with vicsek interaction: 601 (10.01%)\n",
      "Number vicsek interactions with more than 1 neighbor: 464 (77.20%)\n"
     ]
    }
   ],
   "source": [
    "interaction_params = {\n",
    "    \"vicsek\": {\n",
    "        \"rate\": rate * np.ones(N),\n",
    "        \"sigma_v\": sigma_v * np.ones(N),\n",
    "        \"sigma_t\": sigma_t * np.ones(N),\n",
    "        \"interaction_length\": L * np.ones(N),\n",
    "    }\n",
    "}\n",
    "\n",
    "sim_v = StochasticFish_IndependentRates(\n",
    "    N=N,\n",
    "    Tmax=Tmax,\n",
    "    arena_size=arena_size,\n",
    "    interaction_params=interaction_params,\n",
    "    vscale=vscale,\n",
    ")\n",
    "sim_v.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time points skipped for subsampling : 60\n",
      "original_frames: 6001\n",
      "original_shape: (36006, 4)\n",
      "resulting_frames: 101\n",
      "resulting_shape: (606, 4)\n",
      "1 grid unit = 0.5 * simulation length unit\n",
      "1 frame interval = 10.0 *  simulation time unit\n",
      "Predictor kwargs:\n",
      "{'pairwiseCopying': {'dt': 1,\n",
      "                     'interaction_length': 40.0,\n",
      "                     'sigma_t': 0.19999999999999998,\n",
      "                     'sigma_v': 2.5},\n",
      " 'velocityDiffusion': {'dt': 1, 'sigma_t': 0.19999999999999998, 'sigma_v': 2.5},\n",
      " 'vicsek': {'dt': 1,\n",
      "            'interaction_length': 40.0,\n",
      "            'sigma_t': 0.19999999999999998,\n",
      "            'sigma_v': 2.5}}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 16:37:42,972 - Local windows completed in 4.44 seconds.\n",
      "2024-10-07 16:37:49,611 - vicsek completed in 6.64 seconds.\n",
      "2024-10-07 16:38:00,952 - pairwiseCopying completed in 11.34 seconds.\n",
      "2024-10-07 16:38:04,135 - velocityDiffusion completed in 3.18 seconds.\n",
      "2024-10-07 16:38:05,561 - nextStep_sublinear completed in 1.43 seconds.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'collab2.foraging.toolkit' has no attribute 'generate_nextStepExponential_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sim_v_object, derivedDF \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictors_from_sim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msim_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes_skipped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleeway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleeway\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_sampling_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_sampling_fraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size_multiplier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscore_kwargs\u001b[49m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of datapoints : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(derivedDF)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 202\u001b[0m, in \u001b[0;36mget_predictors_from_sim\u001b[0;34m(sim, grid_size, frames_skipped, leeway, verbose, grid_sampling_fraction, normalize, window_size_multiplier, score_kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# derive predictors and scores\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m derivedDF \u001b[38;5;241m=\u001b[39m \u001b[43mderive_predictors_and_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforagers_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_windows_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_windows_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_scaled_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m foragers_object, derivedDF\n",
      "File \u001b[0;32m~/Documents/Repositories/collab-creatures/collab2/foraging/toolkit/derive.py:149\u001b[0m, in \u001b[0;36mderive_predictors_and_scores\u001b[0;34m(foragers_object, local_windows_kwargs, predictor_kwargs, score_kwargs, dropna, add_scaled_values)\u001b[0m\n\u001b[1;32m    147\u001b[0m score_type \u001b[38;5;241m=\u001b[39m score_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    148\u001b[0m function_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_score\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 149\u001b[0m generate_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mftk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    151\u001b[0m derived_quantities[score_name] \u001b[38;5;241m=\u001b[39m generate_function(foragers_object, score_name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'collab2.foraging.toolkit' has no attribute 'generate_nextStepExponential_score'"
     ]
    }
   ],
   "source": [
    "sim_v_object, derivedDF = get_predictors_from_sim(\n",
    "    sim_v,\n",
    "    grid_size=grid_size,\n",
    "    frames_skipped=None,\n",
    "    leeway=leeway,\n",
    "    verbose=True,\n",
    "    grid_sampling_fraction=grid_sampling_fraction,\n",
    "    window_size_multiplier=window_size_multiplier,\n",
    "    score_kwargs = score_kwargs\n",
    "\n",
    ")\n",
    "print(f\"Number of datapoints : {len(derivedDF)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
