{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning functions\n",
    "# remove big jumps\n",
    "def remove_jumps(tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores, max_jump=10):\n",
    "    \"\"\"Removes jumps in the data that are larger than max_jump.\"\"\"\n",
    "    Y = tracks.T\n",
    "    num_frames, num_body_parts, _, num_tracks = Y.shape\n",
    "    new_tracks = []\n",
    "    new_instance_scores = []\n",
    "    new_point_scores = []\n",
    "    new_tracking_scores = []\n",
    "    new_occupancy = []\n",
    "\n",
    "    def diffs_to_next_non_nan(y):\n",
    "        \"\"\"Calculates the difference between each point and the next non-NaN point.\"\"\"\n",
    "        # first calculate the normal diffs\n",
    "        diffs = np.diff(y)\n",
    "        # then replace the diffs after a NaN with the diff to the next non-NaN\n",
    "        nan_indices = np.where(np.isnan(diffs))[0]\n",
    "        for idx in nan_indices:\n",
    "            # check if there's a non-NaN value after the NaN\n",
    "            if np.sum(~np.isnan(y[idx+1:])) > 0:\n",
    "                # if there is, find the index of the next non-NaN value\n",
    "                next_non_nan_idx = np.where(~np.isnan(y[idx+1:]))[0][0] + idx + 1\n",
    "                # replace the NaN diff with the diff to the next non-NaN value\n",
    "                diffs[idx] = y[next_non_nan_idx] - y[idx]\n",
    "        return diffs\n",
    "\n",
    "    for track in range(num_tracks):\n",
    "\n",
    "        # calculate the distance between each point and the next non-NaN point\n",
    "        diffs_x = diffs_to_next_non_nan(Y[:, 0, 0, track])\n",
    "        diffs_y = diffs_to_next_non_nan(Y[:, 0, 1, track])\n",
    "        diffs = np.sqrt(diffs_x**2 + diffs_y**2)\n",
    "            \n",
    "        # Identify where the jumps exceed the threshold\n",
    "        jump_indices = np.where(diffs > max_jump)[0]\n",
    "        for idx in jump_indices:\n",
    "            # Create a new track with initial part of the original track\n",
    "            new_track = Y[:, :, :, track]; #np.full((num_frames, num_body_parts, 2), np.nan)\n",
    "            new_track[idx+1:] = np.nan# Y[idx+1:, :, :, track]\n",
    "            \n",
    "            # Set the original track values before the break point to NaN\n",
    "            Y[:idx+1, :, :, track] = np.nan\n",
    "\n",
    "            # Append new track and its scores\n",
    "            new_tracks.append(new_track)\n",
    "            new_instance_scores.append(instance_scores[track])\n",
    "            new_point_scores.append(point_scores[track])\n",
    "            new_tracking_scores.append(tracking_scores[track])\n",
    "            new_occupancy.append((~np.isnan(new_track[:, 0, 0])).astype(int))\n",
    "          \n",
    "    display('removing jumps from ' + str(len(new_tracks)) + ' tracks')\n",
    "    if new_tracks:\n",
    "        # Append new tracks to Y and update other variables\n",
    "        Y = np.concatenate([Y, np.array(new_tracks).transpose(1,2,3,0)], axis=3)\n",
    "        instance_scores = np.vstack([instance_scores, new_instance_scores])\n",
    "        point_scores = np.concatenate([point_scores, new_point_scores], axis=0)\n",
    "        tracking_scores = np.vstack([tracking_scores, new_tracking_scores])\n",
    "\n",
    "        track_occupancy = np.hstack([track_occupancy, np.array(new_occupancy).transpose()])\n",
    "        track_names.extend([f'track_{i}' for i in range(num_tracks, num_tracks + len(new_tracks))])\n",
    "\n",
    "    tracks = Y.T\n",
    "    return tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores\n",
    "\n",
    "\n",
    "# merge tracks that are close together in time and space\n",
    "def combine_adjacent_tracks(tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores, max_distance=10, max_time=10):\n",
    "    \"\"\"Combines tracks that are close together in time and space.\"\"\"\n",
    "    Y = tracks.T\n",
    "    num_frames, num_body_parts, _, num_tracks = Y.shape\n",
    "\n",
    "    # We'll keep a list of tracks that should be removed after merging\n",
    "    tracks_to_remove = []\n",
    "\n",
    "    for track1 in range(num_tracks):\n",
    "        if np.sum(~np.isnan(Y[:, 0, 0, track1])) == 0:\n",
    "            continue\n",
    "        end_of_track1 = np.where(~np.isnan(Y[:, 0, 0, track1]))[0][-1]\n",
    "        pos_end_track1 = Y[end_of_track1, 0, :, track1]\n",
    "\n",
    "        for track2 in range(track1+1, num_tracks): # Check subsequent tracks\n",
    "            if np.sum(~np.isnan(Y[:, 0, 0, track2])) == 0:\n",
    "                continue\n",
    "            start_of_track2 = np.where(~np.isnan(Y[:, 0, 0, track2]))[0][0]\n",
    "            pos_start_track2 = Y[start_of_track2, 0, :, track2]\n",
    "\n",
    "            time_diff = start_of_track2 - end_of_track1\n",
    "            spatial_diff = np.linalg.norm(pos_end_track1 - pos_start_track2)\n",
    "\n",
    "            if time_diff <= max_time and spatial_diff <= max_distance and time_diff>0:\n",
    "                # Insert the second track into the first track and set the second track to NaN\n",
    "                Y[start_of_track2:, :, :, track1] = Y[start_of_track2:, :, :, track2]\n",
    "                Y[:, :, :, track2] = np.nan\n",
    "                tracks_to_remove.append(track2)\n",
    "    # Remove the merged tracks\n",
    "    tracks = np.delete(tracks, tracks_to_remove, axis=0)\n",
    "    instance_scores = np.delete(instance_scores, tracks_to_remove, axis=0)\n",
    "    point_scores = np.delete(point_scores, tracks_to_remove, axis=0)\n",
    "    track_names = np.delete(np.array(track_names), tracks_to_remove).tolist()\n",
    "    track_occupancy = np.delete(track_occupancy, tracks_to_remove, axis=1)\n",
    "    tracking_scores = np.delete(tracking_scores, tracks_to_remove, axis=0)\n",
    "    display('merging in ' + str(len(tracks_to_remove)) + ' tracks')\n",
    "    return tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores\n",
    "\n",
    "\n",
    "\n",
    "# remove tracks that are too short\n",
    "def remove_short_tracks(tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores, min_length=10):\n",
    "    \"\"\"Removes tracks that are shorter than min_length.\"\"\"\n",
    "    Y = tracks.T\n",
    "    num_frames, num_body_parts, _, num_tracks = Y.shape\n",
    "\n",
    "    # Create a mask to identify tracks that need to be retained\n",
    "    retain_mask = np.ones(num_tracks, dtype=bool)\n",
    "\n",
    "    for track in range(num_tracks):\n",
    "        # Identify where the track is not NaN\n",
    "        track_indices = np.where(~np.isnan(Y[:, 0, 0, track]))[0]\n",
    "        # If the track is shorter than the threshold, mark it for deletion\n",
    "        if len(track_indices) < min_length:\n",
    "            retain_mask[track] = False\n",
    "    display('removing '+str(np.sum(~retain_mask))+ ' tracks, out of ' + str(num_tracks))\n",
    "    # Use the retain_mask to filter out short tracks from all variables\n",
    "    tracks = tracks[retain_mask]\n",
    "    instance_scores = instance_scores[retain_mask]\n",
    "    point_scores = point_scores[retain_mask]\n",
    "    track_names = np.array(track_names)[retain_mask].tolist()\n",
    "    track_occupancy = track_occupancy[:, retain_mask]\n",
    "    tracking_scores = tracking_scores[retain_mask]\n",
    "    return tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# interpolate missing values\n",
    "def fill_missing(tracks, kind=\"linear\"):\n",
    "    \"\"\"Fills missing values independently along each dimension after the first.\"\"\"\n",
    "    Y = tracks.T\n",
    "    # Store initial shape.\n",
    "    initial_shape = Y.shape\n",
    "\n",
    "    # Flatten after first dim.\n",
    "    Y = Y.reshape((initial_shape[0], -1))\n",
    "\n",
    "    # Interpolate along each slice.\n",
    "    for i in range(Y.shape[-1]):\n",
    "        y = Y[:, i]\n",
    "        \n",
    "        # If no valid data or only one data point, skip this iteration\n",
    "        if np.flatnonzero(~np.isnan(y)).size <= 1:\n",
    "            continue\n",
    "\n",
    "        # Build interpolant.\n",
    "        x = np.flatnonzero(~np.isnan(y))\n",
    "        f = interp1d(x, y[x], kind=kind, fill_value=np.nan, bounds_error=False)\n",
    "\n",
    "        # Fill missing\n",
    "        xq = np.flatnonzero(np.isnan(y))\n",
    "        y[xq] = f(xq)\n",
    "\n",
    "        # Save slice\n",
    "        Y[:, i] = y\n",
    "\n",
    "    # Restore to initial shape.\n",
    "    Y = Y.reshape(initial_shape)\n",
    "\n",
    "    tracks = Y.T\n",
    "    display('interpolated missing points')\n",
    "    return tracks\n",
    "\n",
    "# smooth tracks\n",
    "def smooth_diff(tracks, smooth_win=25, smooth_poly=3):\n",
    "    \"\"\"Smooths tracks using a Savitzky-Golay filter.\"\"\"\n",
    "    # Ensure the window size is odd\n",
    "    if smooth_win % 2 == 0:\n",
    "        display('warning: window should be odd')\n",
    "        smooth_win += 1\n",
    "    \n",
    "    Y = tracks.T\n",
    "    num_frames, num_body_parts, _, num_tracks = Y.shape\n",
    "\n",
    "    for track in range(num_tracks):\n",
    "        for part in range(num_body_parts):\n",
    "            for dim in range(2): # x and y\n",
    "                valid_indices = ~np.isnan(Y[:, part, dim, track])\n",
    "                # Only smooth if there are any valid values for the track\n",
    "                if np.any(valid_indices):\n",
    "                    valid_data = Y[valid_indices, part, dim, track]\n",
    "                    smooth_win_use = np.min([smooth_win, len(valid_data)])\n",
    "                    # Apply the filter\n",
    "                    Y[valid_indices, part, dim, track] = savgol_filter(valid_data, smooth_win_use, smooth_poly)\n",
    "    display('smoothed tracks')\n",
    "    return Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'removing jumps from 93 tracks'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'merging in 89 tracks'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'removing 209 tracks, out of 293'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'interpolated missing points'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'saved a total of 84 tracks'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Original filename and the new filename\n",
    "# filename = \"/Users/emily/data/FLIRcam/ForTracking/use_me/hand_checked_20221215122046189_-5_25_bone.avi.predictions.000_20221215122046189_-5_25_bone.analysis.h5\"\n",
    "filename = \"/Users/emily/data/FLIRcam/ForTracking/use_me/20221229124843603_n5_25_bone.avi.predictions.000_20221229124843603_n5_25_bone.analysis.h5\"\n",
    "base, extension = filename.rsplit('.', 1)\n",
    "new_filename = f\"{base}_cleaned.{extension}\"\n",
    "\n",
    "# Open the original file in read mode and the new file in write mode\n",
    "# Open the original file in read mode\n",
    "with h5py.File(filename, 'r') as f, h5py.File(new_filename, 'w') as g:\n",
    "\n",
    "    # Copy all datasets from the old file to the new file\n",
    "    for name, item in f.items():\n",
    "        f.copy(item, g, name)\n",
    "\n",
    "    # Load data from the new file\n",
    "    dset_names = list(g.keys())\n",
    "    tracks = g[\"tracks\"][:]\n",
    "    instance_scores = f[\"instance_scores\"][:]\n",
    "    point_scores = f[\"point_scores\"][:]\n",
    "    track_names = [name.decode() for name in f[\"track_names\"][:]]\n",
    "    track_occupancy = f[\"track_occupancy\"][:]\n",
    "    tracking_scores = f[\"tracking_scores\"][:]\n",
    "\n",
    "    # # Clean data....\n",
    "    \n",
    "    # # remove big jumps\n",
    "    max_jump = 50\n",
    "    tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores = remove_jumps(tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores, max_jump=max_jump)\n",
    "    \n",
    "    # # merge tracks that are close together in time and space\n",
    "    max_distance = 5\n",
    "    max_time = 50\n",
    "    tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores = combine_adjacent_tracks(tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores, max_distance=max_distance, max_time=max_time)\n",
    "    \n",
    "    # remove tracks that are too short\n",
    "    min_length = 10\n",
    "    tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores = remove_short_tracks(tracks, instance_scores, point_scores, track_names, track_occupancy, tracking_scores, min_length=min_length)\n",
    "   \n",
    "    # interpolate missing values\n",
    "    tracks = fill_missing(tracks)\n",
    "\n",
    "    # smooth tracks\n",
    "    tracks = smooth_diff(tracks, smooth_win=15, smooth_poly=3)\n",
    "\n",
    "    # Delete old datasets in new file\n",
    "    del g[\"tracks\"]\n",
    "    del g[\"instance_scores\"]\n",
    "    del g[\"point_scores\"]\n",
    "    del g[\"track_names\"]\n",
    "    del g[\"track_occupancy\"]\n",
    "    del g[\"tracking_scores\"]\n",
    "\n",
    "    # Create new datasets with modified data\n",
    "    g.create_dataset(\"tracks\", data=tracks)\n",
    "    g.create_dataset(\"instance_scores\", data=instance_scores)\n",
    "    g.create_dataset(\"point_scores\", data=point_scores)\n",
    "    g.create_dataset(\"track_names\", data=track_names, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "    g.create_dataset(\"track_occupancy\", data=track_occupancy)\n",
    "    g.create_dataset(\"tracking_scores\", data=tracking_scores)\n",
    "    display('saved a total of '+str(len(track_names))+ ' tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved a total of 87tracks'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    display('saved a total of '+str(len(track_names))+ ' tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fields_recursive(group, indent=0):\n",
    "    \"\"\"Recursively print datasets and groups in an h5py group.\"\"\"\n",
    "    items = sorted(group.items())\n",
    "    for name, item in items:\n",
    "        if isinstance(item, h5py.Dataset):  # Check if item is a dataset\n",
    "            print(\"  \" * indent + f\"Dataset: {name} {item.shape}\")\n",
    "        elif isinstance(item, h5py.Group):  # Check if item is a group\n",
    "            print(\"  \" * indent + f\"Group: {name}\")\n",
    "            print_fields_recursive(item, indent + 1)\n",
    "\n",
    "# filename = 'path_to_your_file.h5'\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    print_fields_recursive(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename, 'r') as f:\n",
    "    instance_scores = f[\"track_occupancy\"][:]\n",
    "instance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('elm_collab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5883c200001963f9c71d117b2067039ffcfe95d2ec18fe3e67d4d08d7043acc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
