{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import dill\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import pyro\n",
    "import foraging_toolkit as ft\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.nn import PyroModule\n",
    "from pyro.infer.autoguide import (\n",
    "    AutoNormal,\n",
    "    AutoDiagonalNormal,\n",
    "    AutoMultivariateNormal,\n",
    "    init_to_mean,\n",
    "    init_to_value,\n",
    ")\n",
    "from pyro.contrib.autoguide import AutoLaplaceApproximation\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import Predictive\n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(message)s\", level=logging.INFO)\n",
    "smoke_test = \"CI\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once and dill\n",
    "# see further comments\n",
    "subset_starts = 420\n",
    "subset_ends = 480\n",
    "\n",
    "#1800 seconds overall\n",
    "\n",
    "locust = ft.load_and_clean_locust(path = \"locust_data/15EQ20191202_tracked.csv\",\n",
    "                                  desired_frames = 900,\n",
    "                                  grid_size=45,\n",
    "                                  rewards_x = [0.68074, -0.69292],\n",
    "                                  rewards_y = [-0.03068, -0.03068],\n",
    "                                  subset_starts= subset_starts,\n",
    "                                  subset_ends = subset_ends,)\n",
    "\n",
    "loc_subset = locust['subset']\n",
    "loc_all = locust['all_frames']\n",
    "\n",
    "loc_all.birdsDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once and dill (below)\n",
    "# to have a re-usable processed dataset\n",
    "# see further comments\n",
    "loc_all = ft.derive_predictors(loc_all, \n",
    "                                        rewards_decay= .4,\n",
    "                                        visibility_range= 90,\n",
    "                                        getting_worse = .001,\n",
    "                                        optimal = 2.11,\n",
    "                                        proximity_decay = 3,\n",
    "                                        generate_communicates=True,\n",
    "                                        info_time_decay=10,\n",
    "                                        info_spatial_decay=0.1,\n",
    "                                        finders_tolerance = 2, \n",
    "                                        time_shift= subset_starts - 1,\n",
    "                                        sampling_rate= .1,\n",
    "                                        restrict_to_invisible= False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data check\n",
    "print(loc_all.derivedDF.shape)\n",
    "loc_all.derivedDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing will take a bit\n",
    "import dill\n",
    "\n",
    "with open(\"locust_data/derived_all.pkl\", \"wb\") as f:\n",
    "    dill.dump(loc_all, f)  \n",
    "\n",
    "# instead of processing anew\n",
    "# you can load preprocessed data\n",
    "# once you have created the file\n",
    "# (file size does not allow for regular GitHub storage)\n",
    "# (WARNING: remember to add to .gitignore to avoid errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once you ran the ove cells once,\n",
    "# you should be using this instead\n",
    "\n",
    "with open(\"/Users/emily/code/collaborative-intelligence/random_hungry_followers/rhf_docs/locust_data/derived_all.pkl\", \"rb\") as f:\n",
    "    loc_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loc_all.derivedDF.shape)\n",
    "loc_all = ft.prep_data_for_robust_inference(loc_all, gridsize= 9)\n",
    "print(loc_all.derivedDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ft.get_tensorized_data(loc_all)\n",
    "proximity, trace, visibility, communicate, how_far = data[\"proximity_standardized\"],  data[\"trace_standardized\"], data[\"visibility\"],  data[\"communicate_standardized\"], data[\"how_far\"]\n",
    "\n",
    "ft.visualise_bird_predictors(trace, proximity, how_far, com = communicate, vis_sampling_rate = .1)\n",
    "# TODO: histogram versions of scatter plots. I suggest modifying ft.visualize_bird_predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locust = loc_all.derivedDF\n",
    "\n",
    "locust[\"proximity_id\"] = locust.proximity_cat.astype(\"category\").cat.codes\n",
    "locust[\"trace_id\"] = locust.trace_cat.astype(\"category\").cat.codes\n",
    "locust[\"communicate_id\"] = locust.communicate_cat.astype(\"category\").cat.codes\n",
    "locust['how_far'] = locust.how_far_squared_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training once, then dill and reuse,\n",
    "# making sure you add to gitignore\n",
    "\n",
    "svi_result_p, svi_result_t, svi_result_c = ft.get_svi_results(locust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same drill: we dill results to avoid repeated computation\n",
    "# make sure you add do git ignore and comment out training\n",
    "# and pickling after running once\n",
    "\n",
    "with open(\"locust_data/svi_results.pkl\", \"wb\") as f:\n",
    "    dill.dump((svi_result_p, svi_result_t, svi_result_c), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading: will work if you run the above training and dilling cells once\n",
    "with open(\"locust_data/svi_results.pkl\", \"rb\") as f:\n",
    "    svi_result_p, svi_result_t, svi_result_c = dill.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup of extreme values resulting from empty cells\n",
    "\n",
    "summary = {}\n",
    "summary[\"id_p\"] = sorted(locust[\"proximity_id\"].unique())\n",
    "summary[\"params_p\"] = svi_result_p.params['auto_loc'][:-1]\n",
    "summary[\"std_p\"] = locust.groupby('proximity_id')['how_far'].std()\n",
    "\n",
    "summary[\"id_t\"] = sorted(locust[\"trace_id\"].unique())[:-1]\n",
    "summary[\"params_t\"] = svi_result_t.params['auto_loc'][:-2]\n",
    "summary[\"std_t\"] = locust.groupby('trace_id')['how_far'].std()[:-1]\n",
    "\n",
    "\n",
    "summary[\"id_c\"] = sorted(locust[\"communicate_id\"].unique())\n",
    "summary[\"params_c\"] = svi_result_c.params['auto_loc'][:-1]\n",
    "summary[\"std_c\"] = locust.groupby('communicate_id')['how_far'].std()\n",
    "\n",
    "\n",
    "assert len(summary[\"id_p\"]) == len(summary[\"params_p\"]) == len(summary[\"std_p\"]), \"Lengths of id_p, params_p, and std_p do not match.\"\n",
    "assert len(summary[\"id_t\"]) == len(summary[\"params_t\"]) == len(summary[\"std_t\"]), \"Lengths of id_t, params_t, and std_t do not match.\"\n",
    "assert len(summary[\"id_c\"]) == len(summary[\"params_c\"]) == len(summary[\"std_c\"]), \"Lengths of id_c, params_c, and std_c do not match.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_p, lr_t, lr_c = [LinearRegression() for _ in range(3)]\n",
    "\n",
    "X_p = np.array(summary[\"id_p\"]).reshape(-1, 1)\n",
    "X_t = np.array(summary[\"id_t\"]).reshape(-1, 1)\n",
    "X_c = np.array(summary[\"id_c\"]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "lr_p.fit(X_p, summary[\"params_p\"], sample_weight=1/summary[\"std_p\"])\n",
    "lr_t.fit(X_t, summary[\"params_t\"], sample_weight=1/summary[\"std_t\"])\n",
    "lr_c.fit(X_c, summary[\"params_c\"], sample_weight=1/summary[\"std_c\"])\n",
    "\n",
    "\n",
    "X_new = np.arange(0, 8.1, 0.1).reshape((-1, 1))\n",
    "p_pred = lr_p.predict(X_new)\n",
    "t_pred = lr_t.predict(X_new)\n",
    "c_pred = lr_c.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_and_plot_coef(coef, input, model):\n",
    "\n",
    "    coef_samples = []\n",
    "    for _ in range(1000):\n",
    "        X_resampled, y_resampled = resample(input, summary[f\"params_{coef}\"], random_state=np.random.randint(1000))\n",
    "    \n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        coef_samples.append(model.coef_[0])\n",
    "\n",
    "\n",
    "    histogram_trace = go.Histogram(\n",
    "    x=coef_samples,\n",
    "    marker=dict(color='blue'),\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title= f'Histogram of coef_samples_{coef}',\n",
    "        xaxis=dict(title='Coefficient Value'),\n",
    "        yaxis=dict(title='Frequency'),\n",
    "        paper_bgcolor='black',  \n",
    "        plot_bgcolor='black',   \n",
    "        font=dict(color='white'), \n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[histogram_trace], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "sample_and_plot_coef(\"p\", X_p, lr_p)\n",
    "sample_and_plot_coef(\"t\", X_t, lr_t)\n",
    "sample_and_plot_coef(\"c\", X_c, lr_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summaries (coef, model , pred, title, raw_predictor, ylim = (0,1.2), xlim = (0,8), vis_sampling_rate = 1):\n",
    "\n",
    "\n",
    "    fig =   px.scatter(\n",
    "            x=summary[f\"id_{coef}\"],\n",
    "            y=summary[f\"params_{coef}\"],\n",
    "            error_y=summary[f\"std_{coef}\"]/2,\n",
    "            opacity=0.9,\n",
    "            template=\"plotly_dark\",\n",
    "        )\n",
    "\n",
    "    fig.add_scatter(x=X_new.flatten(), y=pred, mode='lines', line=dict(color='red', width=2),\n",
    "                    showlegend=False)\n",
    "    \n",
    "\n",
    "    def sample_and_scale_vector(vector, vis_sampling_rate, xlim):\n",
    "        sample_size = int(vis_sampling_rate * len(vector))\n",
    "        sampled_vector = np.random.choice(vector, size=sample_size, replace=False)\n",
    "        scaled_vector = (sampled_vector - xlim[0]) *  (xlim[1] - xlim[0]) + xlim[0]\n",
    "        return scaled_vector\n",
    "\n",
    "    raw_predictor_vector = sample_and_scale_vector(locust[raw_predictor], vis_sampling_rate, xlim)\n",
    "        \n",
    "\n",
    "    fig.add_scatter(x=raw_predictor_vector, y=locust[\"how_far\"], mode='markers', opacity=0.2, marker=dict(color='magenta', size = 3),\n",
    "                    showlegend=False)\n",
    "\n",
    "    fig.update_layout(\n",
    "            title=f\"{title}: estimated means, weighted linear model (w = {np.round(model.coef_, decimals=3)}) and raw data variances\",\n",
    "            xaxis_title=\"predictor\",\n",
    "            yaxis_title=\"how far score\",\n",
    "        )\n",
    "    \n",
    "        \n",
    "    if ylim:\n",
    "        fig.update_yaxes(range=ylim)\n",
    "    if xlim:\n",
    "        fig.update_xaxes(range=xlim)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_summaries(\"p\", lr_p, p_pred, raw_predictor= \"proximity_standardized\", title =  \"Proximity\", ylim = (0,1), vis_sampling_rate= .1)\n",
    "plot_summaries(\"t\", lr_t, t_pred, raw_predictor = \"trace_standardized\", title = \"Trace\", ylim = (0,1), xlim= (0,6), vis_sampling_rate = .1)\n",
    "plot_summaries(\"c\", lr_c, c_pred, raw_predictor = \"communicate_standardized\", title = \"Communicate\", ylim = (0,1), vis_sampling_rate = .1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('chirho')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ced61df5fd1b571571cf668ec5f5c73de50ba198f06f1947685633a6f589aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
